{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":82695,"databundleVersionId":9738540},{"sourceType":"datasetVersion","sourceId":9094368,"datasetId":5251603,"databundleVersionId":9268800},{"sourceType":"datasetVersion","sourceId":9387960,"datasetId":5688622,"databundleVersionId":9589221},{"sourceType":"datasetVersion","sourceId":10469779,"datasetId":4581967,"databundleVersionId":10793318},{"sourceType":"datasetVersion","sourceId":8897601,"datasetId":5297895,"databundleVersionId":9058412},{"sourceType":"datasetVersion","sourceId":9948011,"datasetId":6117312,"databundleVersionId":10210204},{"sourceType":"datasetVersion","sourceId":8218776,"datasetId":4871830,"databundleVersionId":8344064},{"sourceType":"modelInstanceVersion","sourceId":174909,"databundleVersionId":10246385,"modelInstanceId":148911},{"sourceType":"modelInstanceVersion","sourceId":174921,"databundleVersionId":10246586,"modelInstanceId":148923},{"sourceType":"modelInstanceVersion","sourceId":118192,"databundleVersionId":9658561,"modelInstanceId":99392},{"sourceType":"kernelVersion","sourceId":200567623},{"sourceType":"kernelVersion","sourceId":208028495},{"sourceType":"kernelVersion","sourceId":212347964}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"toc_visible":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["<center><h1><b>ĐỒ ÁN: EEDI - MINING MISCONCEPTIONS IN MATHEMATICS</b></h1><center>"],"metadata":{"id":"kh62TL85wLaD"}},{"cell_type":"markdown","source":["<center><h2><u>Môn:</u> Học sâu cho Khoa học dữ liệu - <u>Lớp:</u> 21_21</h2></center>\n","\n","<h3><u>Giáo viên hướng dẫn:</u><h3>\n","\n","- TS. Nguyễn Tiến Huy\n","- TS. Lê Thanh Tùng\n","- ThS. Nguyễn Trần Duy Minh  \n","\n","<h3><u>Nhóm thực hiện:</u> Nhóm 1</h3>\n","\n","| Họ và tên | MSSV |\n","| :--- | :---:|\n","| Phạm Nhật Duy | 21120058 |\n","| Nguyễn Trúc Nguyên | 21120102 |\n","| Trương Công Trung | 21120158 |\n","| Lê Trần Minh Khuê | 21120279 |"],"metadata":{"id":"MmGdZnfpwLaF"}},{"cell_type":"markdown","source":["# **Tổng quan**\n","\n","<u>Trong phạm vi đồ án này</u>, nhóm chúng em sẽ phát triển một mô hình học máy cung cấp khả năng phân tích và dự đoán mối liên hệ giữa *các ngộ nhận* với *các câu trả lời không chính xác* trong các câu hỏi trắc nghiệm.\n","\n","<u>Mục tiêu:</u> Đối với mỗi câu hỏi trắc nghiệm, mô hình sẽ\n","\n","- Xác định được câu trả lời đúng,\n","- Xác định được những lầm tưởng/ ngộ nhận có thể dẫn đến việc chọn đáp án sai.\n","\n","<u>Trong phạm vi file này</u>, nhóm sẽ tiến hành xây dựng quy trình áp dụng mô hình ngôn ngữ lớn để xác định những ngộ nhận có thể dẫn đến việc chọn đáp án sai từ bộ dữ liệu Eedi.\n"],"metadata":{"id":"8tWDuAcswLaI"}},{"cell_type":"markdown","source":["# **First retrieval - Retrieve**"],"metadata":{"id":"loYK_cQHwLaK"}},{"cell_type":"code","source":["!pip install /kaggle/input/k/justdylan/eedi-library/peft-0.14.0-py3-none-any.whl  --no-index --find-links=/kaggle/input/eedi-library --quiet"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:03.672070Z","iopub.execute_input":"2025-01-14T17:16:03.672416Z","iopub.status.idle":"2025-01-14T17:16:13.845095Z","shell.execute_reply.started":"2025-01-14T17:16:03.672386Z","shell.execute_reply":"2025-01-14T17:16:13.843968Z"},"id":"Vd_GvuiKwLaM"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install /kaggle/input/eedi-library-from-sinchiro/autoawq-0.2.7.post1-py3-none-any.whl  --no-index --find-links=/kaggle/input/eedi-library-from-sinchiro --quiet"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:13.847332Z","iopub.execute_input":"2025-01-14T17:16:13.847620Z","iopub.status.idle":"2025-01-14T17:16:31.543156Z","shell.execute_reply.started":"2025-01-14T17:16:13.847592Z","shell.execute_reply":"2025-01-14T17:16:31.541450Z"},"id":"Nj2mAKRswLaN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["import os, math, numpy as np\n","import sys\n","from transformers import AutoTokenizer\n","import pandas as pd\n","from tqdm import tqdm\n","import re, gc\n","import torch\n","pd.set_option('display.max_rows', 300)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""],"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:31.545557Z","iopub.execute_input":"2025-01-14T17:16:31.546060Z","iopub.status.idle":"2025-01-14T17:16:37.715880Z","shell.execute_reply.started":"2025-01-14T17:16:31.546020Z","shell.execute_reply":"2025-01-14T17:16:37.714819Z"},"id":"NPjU2geFwLaR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["IS_SUBMISSION = True\n","\n","MISCONCEPTION_MAPPING_PATH = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n","TRAIN_DATA_PATH = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\"\n","TEST_DATA_PATH = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\"\n","\n","LORA_PATH = '/kaggle/input/2211-lora-14b/transformers/default/1'\n","MODEL_PATH = \"/kaggle/input/qw14b-awq/transformers/default/1\""],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:37.718659Z","iopub.execute_input":"2025-01-14T17:16:37.719572Z","iopub.status.idle":"2025-01-14T17:16:37.724551Z","shell.execute_reply.started":"2025-01-14T17:16:37.719513Z","shell.execute_reply":"2025-01-14T17:16:37.723546Z"},"id":"Ww4XSeq_wLaT"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["print('IS_SUBMISSION:', IS_SUBMISSION)\n","\n","df_misconception_mapping = pd.read_csv(MISCONCEPTION_MAPPING_PATH)\n","\n","if not IS_SUBMISSION:\n","    df_ret = pd.read_csv(TRAIN_DATA_PATH).fillna(-1).sample(100, random_state=103).reset_index(drop=True)\n","else:\n","    df_ret = pd.read_csv(TEST_DATA_PATH)\n","\n","df_ret"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:37.725956Z","iopub.execute_input":"2025-01-14T17:16:37.726243Z","iopub.status.idle":"2025-01-14T17:16:40.578935Z","shell.execute_reply.started":"2025-01-14T17:16:37.726215Z","shell.execute_reply":"2025-01-14T17:16:40.577918Z"},"id":"BXM5HLRgwLaU","outputId":"8a39ebec-069f-416b-da1c-569544a37fb7"},"outputs":[{"name":"stdout","text":"IS_SUBMISSION: True\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   QuestionId  ConstructId                                      ConstructName  \\\n0        1869          856  Use the order of operations to carry out calcu...   \n1        1870         1612  Simplify an algebraic fraction by factorising ...   \n2        1871         2774            Calculate the range from a list of data   \n\n   SubjectId                                        SubjectName CorrectAnswer  \\\n0         33                                             BIDMAS             A   \n1       1077                    Simplifying Algebraic Fractions             D   \n2        339  Range and Interquartile Range from a List of Data             B   \n\n                                        QuestionText            AnswerAText  \\\n0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n2  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n\n              AnswerBText            AnswerCText             AnswerDText  \n0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets  \n1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify  \n2             Only\\nKatie     Both Tom and Katie      Neither is correct  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>QuestionId</th>\n      <th>ConstructId</th>\n      <th>ConstructName</th>\n      <th>SubjectId</th>\n      <th>SubjectName</th>\n      <th>CorrectAnswer</th>\n      <th>QuestionText</th>\n      <th>AnswerAText</th>\n      <th>AnswerBText</th>\n      <th>AnswerCText</th>\n      <th>AnswerDText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1869</td>\n      <td>856</td>\n      <td>Use the order of operations to carry out calcu...</td>\n      <td>33</td>\n      <td>BIDMAS</td>\n      <td>A</td>\n      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n      <td>\\( 3 \\times(2+4)-5 \\)</td>\n      <td>\\( 3 \\times 2+(4-5) \\)</td>\n      <td>\\( 3 \\times(2+4-5) \\)</td>\n      <td>Does not need brackets</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1870</td>\n      <td>1612</td>\n      <td>Simplify an algebraic fraction by factorising ...</td>\n      <td>1077</td>\n      <td>Simplifying Algebraic Fractions</td>\n      <td>D</td>\n      <td>Simplify the following, if possible: \\( \\frac{...</td>\n      <td>\\( m+1 \\)</td>\n      <td>\\( m+2 \\)</td>\n      <td>\\( m-1 \\)</td>\n      <td>Does not simplify</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1871</td>\n      <td>2774</td>\n      <td>Calculate the range from a list of data</td>\n      <td>339</td>\n      <td>Range and Interquartile Range from a List of Data</td>\n      <td>B</td>\n      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n      <td>Only\\nTom</td>\n      <td>Only\\nKatie</td>\n      <td>Both Tom and Katie</td>\n      <td>Neither is correct</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["+ Đầu tiện cần định dạng dữ liệu bằng cách tạo ra một cấu trúc câu hỏi chuẩn hóa với đầy đủ thông tin, bao gồm:\n","  + Nội dung câu hỏi.\n","  + Câu trả lời đúng.\n","  + Câu trả lời sai mà học sinh chọn.\n","+ Sau đó tạo một chuỗi đầu vào (prompt) cho mô hình ngôn ngữ lớn để phân tích và dự đoán."],"metadata":{"id":"LoJ6qM6BwLaX"}},{"cell_type":"code","source":["def format_input(row, wrong_choice):\n","    assert wrong_choice in \"ABCD\"\n","    # Extract values from the row\n","    question_text = row.get(\"QuestionText\", \"No question text provided\")\n","    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n","    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n","\n","    # Extract the correct and wrong answer text based on the choice\n","    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n","    assert wrong_choice != correct_answer\n","    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n","    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n","\n","    # Construct the question format\n","    formatted_question = f\"\"\"Question: {question_text}\n","\n","SubjectName: {subject_name}\n","ConstructName: {construct_name}\"\"\"\n","\n","    TEMPLATE_INPUT = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n","\n","    # Return the extracted data\n","    ret = {\n","        \"QUESTION\": formatted_question,\n","        \"CORRECT_ANSWER\": correct_answer_text,\n","        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n","        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n","    }\n","    ret[\"PROMPT\"] = TEMPLATE_INPUT.format(**ret)\n","\n","    return ret"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:40.580139Z","iopub.execute_input":"2025-01-14T17:16:40.580414Z","iopub.status.idle":"2025-01-14T17:16:40.587782Z","shell.execute_reply.started":"2025-01-14T17:16:40.580386Z","shell.execute_reply":"2025-01-14T17:16:40.586807Z"},"id":"U672xE5-wLaX"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["items = []\n","rows = []\n","for _, row in df_ret.iterrows():\n","    for choice in ['A', 'B', 'C', 'D']:\n","        if choice == row[\"CorrectAnswer\"]:\n","            continue\n","\n","        correct_answer = row[f\"Answer{row['CorrectAnswer']}Text\"]\n","\n","        item = {\n","            'QuestionId_Answer': f\"{row['QuestionId']}_{choice}\",\n","            'Prompt': format_input(row, choice)['PROMPT']\n","        }\n","        items.append(item)\n","\n","        query_text = (f\"Subject name: {row['SubjectName']}\\n\"\n","                      f\"Construct name: {row['ConstructName']}\\n\"\n","                      f\"Question: {row['QuestionText']}\\n\"\n","                      f\"Correct answer: {correct_answer}\\n\"\n","                      f\"### Misconcepte incorrect answer: {choice}.{row[f'Answer{choice}Text']}\")\n","\n","        rows.append({\n","            \"query_text\": query_text,\n","            \"QuestionId_Answer\": f\"{row['QuestionId']}_{choice}\",\n","            \"ConstructName\": row[\"ConstructName\"],\n","            \"SubjectName\": row[\"SubjectName\"],\n","            \"QuestionText\": row[\"QuestionText\"],\n","            \"correct_answer\": correct_answer,\n","            \"incorrect_answer\": row[f\"Answer{choice}Text\"]\n","        })\n","\n","df_input = pd.DataFrame(items)\n","df = pd.DataFrame(rows)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:40.589082Z","iopub.execute_input":"2025-01-14T17:16:40.589368Z","iopub.status.idle":"2025-01-14T17:16:40.603134Z","shell.execute_reply.started":"2025-01-14T17:16:40.589341Z","shell.execute_reply":"2025-01-14T17:16:40.602287Z"},"id":"oSTYpdXWwLaY"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["+ Tạo các truy vấn chi tiết từ dữ liệu đầu vào. Sau đó, xử lý truy vấn để tạo nhúng (`embeddings`) bằng cách chuẩn hóa và định dạng lại truy vấn để phù hợp với mô hình ngôn ngữ lớn bao gồm cắt ngắn nếu vượt quá độ dài tối đa (`query_max_len`) và thêm tiền tố/hậu tố vào truy vấn."],"metadata":{"id":"fPIvtG6cwLaZ"}},{"cell_type":"code","source":["def get_detailed_instruct(task_description: str, query: str) -> str:\n","    return f'<instruct>{task_description}\\n<query>{query}'\n","\n","def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n","    inputs = tokenizer(\n","        queries,\n","        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n","            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n","        return_token_type_ids=False,\n","        truncation=True,\n","        return_tensors=None,\n","        add_special_tokens=False\n","    )\n","    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n","    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n","    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n","    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n","    for i in range(len(new_queries)):\n","        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n","    return new_max_length, new_queries\n","\n","task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n","queries = [\n","    get_detailed_instruct(task, q) for q in df_input['Prompt']\n","]\n","documents = df_misconception_mapping['MisconceptionName'].tolist()\n","query_max_len, doc_max_len = 320, 48\n","tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n","examples_prefix = ''\n","new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n","\n","import json\n","with open('data.json', 'w') as f:\n","    data = {'texts': new_queries + documents}\n","    f.write(json.dumps(data))"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:40.604198Z","iopub.execute_input":"2025-01-14T17:16:40.604673Z","iopub.status.idle":"2025-01-14T17:16:53.822049Z","shell.execute_reply.started":"2025-01-14T17:16:40.604645Z","shell.execute_reply":"2025-01-14T17:16:53.821135Z"},"id":"Bc7gpxxTwLaZ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["+ Tiếp theo tạo vector embedding cho các đoạn văn bản đầu vào sử dụng mô hình ngôn ngữ lớn.\n","+ Bên cạnh đó áp dụng kỹ thuật LoRA (Low-Rank Adaptation) để giảm tải bộ nhớ khi huấn luyện hoặc inferencing với mô hình lớn."],"metadata":{"id":"oYLCcepSwLaa"}},{"cell_type":"code","source":["%%writefile run_embed.py\n","import argparse\n","import os\n","import json\n","import torch\n","import torch.nn.functional as F\n","from torch import Tensor\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","import peft\n","\n","MAX_LENGTH = 320\n","\n","def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n","    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n","    if left_padding:\n","        return last_hidden_states[:, -1]\n","    else:\n","        sequence_lengths = attention_mask.sum(dim=1) - 1\n","        batch_size = last_hidden_states.shape[0]\n","        return last_hidden_states[\n","            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n","        ]\n","\n","def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n","    embeddings = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n","        batch_texts = texts[i : i + batch_size]\n","        batch_dict = tokenizer(\n","            batch_texts,\n","            max_length=max_length,\n","            padding=True,\n","            truncation=True,\n","            return_tensors=\"pt\",\n","        ).to(\"cuda\")\n","        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n","            outputs = model(**batch_dict)\n","            batch_embeddings = last_token_pool(\n","                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n","            )\n","            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n","        embeddings.append(batch_embeddings)\n","    return torch.cat(embeddings, dim=0)\n","\n","def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n","    model = AutoModel.from_pretrained(\n","        base_model_path,\n","        device_map=0,\n","        torch_dtype=torch.float16,\n","        load_in_4bit=load_in_4bit,\n","    )\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        lora_path if lora_path else base_model_path\n","    )\n","    model.resize_token_embeddings(len(tokenizer))\n","    if lora_path:\n","        model = peft.PeftModel.from_pretrained(model, lora_path)\n","    return model, tokenizer\n","\n","def main(args):\n","    output_file = args.input_text.replace(\n","        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n","    )\n","    if os.path.exists(output_file):\n","        print(f\"Output file {output_file} already exists. Skipping...\")\n","        return\n","    model, tokenizer = load_model_and_tokenizer(\n","        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n","    )\n","    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n","    embeddings = get_embeddings_in_batches(\n","        model,\n","        tokenizer,\n","        texts,\n","        max_length=MAX_LENGTH,\n","        batch_size=4,\n","    )\n","    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n","    torch.save(text2embeds, output_file)\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"--base_model\",\n","        type=str,\n","        default=\"Qwen/Qwen2.5-14B\",\n","        help=\"Path to the base model\",\n","    )\n","    parser.add_argument(\n","        \"--lora_path\",\n","        type=str,\n","        default=None,\n","        help=\"Path to the LoRA model\",\n","    )\n","    parser.add_argument(\n","        \"--input_text\",\n","        type=str,\n","        default=\".cache/data.json\",\n","    )\n","    parser.add_argument(\n","        \"--load_in_4bit\",\n","        action=\"store_true\",\n","        help=\"Load model in 4-bit mode\",\n","    )\n","    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n","    args = parser.parse_args()\n","    if not os.path.exists(args.lora_path):\n","        args.lora_path = None\n","    main(args)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:53.823334Z","iopub.execute_input":"2025-01-14T17:16:53.823892Z","iopub.status.idle":"2025-01-14T17:16:53.831364Z","shell.execute_reply.started":"2025-01-14T17:16:53.823861Z","shell.execute_reply":"2025-01-14T17:16:53.830504Z"},"id":"is-6TFz7wLaa","outputId":"a2210321-4e4b-49b4-b6f5-e302ac891946"},"outputs":[{"name":"stdout","text":"Writing run_embed.py\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["+ Xử lý dữ liệu theo từng phân đoạn để tối ưu bộ nhớ và hiệu năng."],"metadata":{"id":"TQ97pdV3wLac"}},{"cell_type":"code","source":["cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model {MODEL_PATH} --lora_path {LORA_PATH} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model {MODEL_PATH} --lora_path {LORA_PATH} --input_text data.json --fold 1 2)\"\n","os.system(cmd)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:16:53.834204Z","iopub.execute_input":"2025-01-14T17:16:53.834662Z","iopub.status.idle":"2025-01-14T17:22:59.757879Z","shell.execute_reply.started":"2025-01-14T17:16:53.834611Z","shell.execute_reply":"2025-01-14T17:22:59.756820Z"},"id":"ksayfC_HwLac","outputId":"dc25b67d-43b3-4510-8d3d-d4db37dc0cdf"},"outputs":[{"name":"stdout","text":"ERROR! Intel® Extension for PyTorch* needs to work with PyTorch 2.5.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\nERROR! Intel® Extension for PyTorch* needs to work with PyTorch 2.5.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\nERROR! Intel® Extension for PyTorch* needs to work with PyTorch 2.5.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\nERROR! Intel® Extension for PyTorch* needs to work with PyTorch 2.5.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\n","output_type":"stream"},{"name":"stderr","text":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/accelerate/utils/imports.py:336: UserWarning: Intel Extension for PyTorch 2.5 needs to work with PyTorch 2.5.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/imports.py:336: UserWarning: Intel Extension for PyTorch 2.5 needs to work with PyTorch 2.5.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 59.11s/it]\nLoading checkpoint shards: 100%|██████████| 2/2 [01:58<00:00, 59.30s/it]\nEmbedding: 100%|██████████| 325/325 [03:13<00:00,  1.68it/s]\nEmbedding: 100%|██████████| 325/325 [03:25<00:00,  1.59it/s]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["+ Cần đảm bảo rằng tất cả các tệp nhúng đã được tạo xong và hợp nhất chúng vào một cấu trúc duy nhất (`text_to_embed`)."],"metadata":{"id":"b7bP-fM_wLad"}},{"cell_type":"code","source":["from glob import glob\n","import time\n","text_to_embed = {}\n","files = glob('*.pt*')\n","while len(files) != 2:\n","    time.sleep(1)\n","    files = glob('*.pt*')\n","\n","time.sleep(3)\n","for path in files:\n","    print(path)\n","    text_to_embed.update(torch.load(path))"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:22:59.759222Z","iopub.execute_input":"2025-01-14T17:22:59.759555Z","iopub.status.idle":"2025-01-14T17:23:02.845887Z","shell.execute_reply.started":"2025-01-14T17:22:59.759524Z","shell.execute_reply":"2025-01-14T17:23:02.844800Z"},"id":"mEimVftewLae","outputId":"08b893b3-af2f-4ced-cce0-494c88d7e378"},"outputs":[{"name":"stdout","text":"data.pt.fold.0.2.embed\ndata.pt.fold.1.2.embed\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3571780413.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  text_to_embed.update(torch.load(path))\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["+ Sử dụng tích vô hướng giữa các vector nhúng để xác định độ tương đồng giữa sai lầm trong câu trả lời (truy vấn) và các ngộ nhận đã có (tài liệu).\n","+ Sau đó lấy danh sách của 25 ngộ nhận phù hợp nhất cho mỗi truy vấn, theo thứ tự giảm dần của điểm tương đồng."],"metadata":{"id":"0pjpCnj9wLae"}},{"cell_type":"code","source":["query_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\n","doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n","query_embeddings.shape, doc_embeddings.shape"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:23:02.847117Z","iopub.execute_input":"2025-01-14T17:23:02.847425Z","iopub.status.idle":"2025-01-14T17:23:02.907733Z","shell.execute_reply.started":"2025-01-14T17:23:02.847396Z","shell.execute_reply":"2025-01-14T17:23:02.906681Z"},"id":"_tRKrBWhwLae","outputId":"15c9b70d-689d-41e6-dc32-95230d0ef64a"},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(torch.Size([9, 5120]), torch.Size([2587, 5120]))"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["scores = query_embeddings @ doc_embeddings.T  # Shape: (M, N)\n","sorted_indices = torch.argsort(scores,1, descending=True)[:,:25].tolist()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:23:02.909067Z","iopub.execute_input":"2025-01-14T17:23:02.909402Z","iopub.status.idle":"2025-01-14T17:23:02.967657Z","shell.execute_reply.started":"2025-01-14T17:23:02.909371Z","shell.execute_reply":"2025-01-14T17:23:02.966699Z"},"id":"RUIDPTe_wLaf"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["+ Để dữ liệu đã được xử lý có thể được sử dụng lại mà không cần thực hiện lại toàn bộ pipeline, ta lưu lại kết quả vào hai tệp để sử dụng trong các bước tiếp theo."],"metadata":{"id":"ATGM7e7cwLaf"}},{"cell_type":"code","source":["np.save(\"indices.npy\", sorted_indices)\n","df.to_parquet(\"df.parquet\", index=False)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:23:02.969390Z","iopub.execute_input":"2025-01-14T17:23:02.969820Z","iopub.status.idle":"2025-01-14T17:23:03.052902Z","shell.execute_reply.started":"2025-01-14T17:23:02.969780Z","shell.execute_reply":"2025-01-14T17:23:03.051810Z"},"id":"jhY02IQRwLaf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:23:03.054225Z","iopub.execute_input":"2025-01-14T17:23:03.054545Z","iopub.status.idle":"2025-01-14T17:23:03.059233Z","shell.execute_reply.started":"2025-01-14T17:23:03.054514Z","shell.execute_reply":"2025-01-14T17:23:03.058084Z"},"id":"Cj8TaDYIwLag"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["---"],"metadata":{"id":"saF5kyCOwLag"}},{"cell_type":"markdown","source":["# **Second retrieval - Rerank**"],"metadata":{"id":"rCTI5iBQwLag"}},{"cell_type":"code","source":["!pip uninstall -y torch --quiet\n","!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm --quiet\n","!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --quiet\n","!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl --quiet\n","!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl --quiet\n","!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl --quiet"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:23:03.060656Z","iopub.execute_input":"2025-01-14T17:23:03.061095Z","iopub.status.idle":"2025-01-14T17:26:58.957633Z","shell.execute_reply.started":"2025-01-14T17:23:03.061045Z","shell.execute_reply":"2025-01-14T17:26:58.956029Z"},"id":"TPcjTOF_wLag","outputId":"5d5ef147-36a4-4f80-f3c5-72f677fb0d3d"},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["!pip install transformers peft accelerate \\\n","    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files --quiet"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:26:58.959399Z","iopub.execute_input":"2025-01-14T17:26:58.959878Z","iopub.status.idle":"2025-01-14T17:27:09.317287Z","shell.execute_reply.started":"2025-01-14T17:26:58.959829Z","shell.execute_reply":"2025-01-14T17:27:09.315981Z"},"id":"pnolKtMpwLah"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["%%capture\n","!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n","!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n","!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:27:09.319281Z","iopub.execute_input":"2025-01-14T17:27:09.319767Z","iopub.status.idle":"2025-01-14T17:27:58.779130Z","shell.execute_reply.started":"2025-01-14T17:27:09.319697Z","shell.execute_reply":"2025-01-14T17:27:58.777670Z"},"id":"A4QkM5ZwwLai"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["+ Tạo prompt cung cấp đầy đủ ngữ cảnh cho mô hình để phân tích và chọn ngộ nhận phù hợp.\n","    + Mô tả ngữ cảnh (`ConstructName` và `SubjectName`).\n","    + Thông tin về câu hỏi, đáp án đúng, và đáp án sai.\n","    + Danh sách các ngộ nhận (`Retrieval`).\n","  \n","+ Tải mô hình ngôn ngữ lớn (với cấu hình `awq` và `16-bit floating-point` để tối ưu cho GPU):\n","    + Xếp hạng các ngộ nhận qua từng vòng lặp.\n","    + Dùng LLM để dự đoán ngộ nhận phù hợp nhất cho từng câu hỏi."],"metadata":{"id":"YpWSFWvWwLai"}},{"cell_type":"code","source":["%%writefile run_vllm.py\n","\n","import vllm\n","import numpy as np\n","import pandas as pd\n","from transformers import PreTrainedTokenizer, AutoTokenizer\n","from typing import List\n","import torch\n","from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n","import re\n","\n","model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","def preprocess_text(x):\n","    x = re.sub(\"http\\w+\", '',x)\n","    x = re.sub(r\"\\.+\", \".\", x)\n","    x = re.sub(r\"\\,+\", \",\", x)\n","    x = re.sub(r\"\\\\\\(\", \" \", x)\n","    x = re.sub(r\"\\\\\\)\", \" \", x)\n","    x = re.sub(r\"[ ]{1,}\", \" \", x)\n","    x = x.strip()\n","    return x\n","\n","PROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n","Question: {Question}\n","Correct Answer: {CorrectAnswer}\n","Incorrect Answer: {IncorrectAnswer}\n","\n","You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n","Answer concisely what misconception it is to lead to getting the Incorrect Answer.\n","Pick the correct misconception number from the below:\n","\n","{Retrieval}\n","\"\"\"\n","\n","def apply_template(row, tokenizer):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": preprocess_text(\n","                PROMPT.format(\n","                    ConstructName=row[\"ConstructName\"],\n","                    SubjectName=row[\"SubjectName\"],\n","                    Question=row[\"QuestionText\"],\n","                    IncorrectAnswer=row[f\"incorrect_answer\"],\n","                    CorrectAnswer=row[f\"correct_answer\"],\n","                    Retrieval=row[f\"retrieval\"]\n","                )\n","            )\n","        }\n","    ]\n","    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    return text\n","\n","df = pd.read_parquet(\"df.parquet\")\n","indices = np.load(\"indices.npy\")\n","df_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n","\n","llm = vllm.LLM(\n","    model_path,\n","    quantization=\"awq\",\n","    tensor_parallel_size=2,\n","    gpu_memory_utilization=0.98,\n","    trust_remote_code=True,\n","    dtype=\"half\",\n","    enforce_eager=True,\n","    max_model_len=5120,\n","    disable_log_stats=True\n",")\n","tokenizer = llm.get_tokenizer()\n","\n","def get_candidates(c_indices):\n","    candidates = []\n","\n","    mis_names = df_misconception_mapping[\"MisconceptionName\"].values\n","    for ix in c_indices:\n","        c_names = []\n","        for i, name in enumerate(mis_names[ix]):\n","            c_names.append(f\"{i+1}. {name}\")\n","\n","        candidates.append(\"\\n\".join(c_names))\n","\n","    return candidates\n","\n","survivors = indices[:, -1:]\n","\n","for i in range(3):\n","    c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n","\n","    df[\"retrieval\"] = get_candidates(c_indices)\n","    df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n","\n","    print(\"Example:\")\n","    print(df[\"text\"].values[0])\n","    print()\n","\n","    responses = llm.generate(\n","        df[\"text\"].values,\n","        vllm.SamplingParams(\n","            n=1,\n","            top_k=1,\n","            temperature=0,\n","            seed=777,\n","            skip_special_tokens=False,\n","            max_tokens=1,\n","            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n","        ),\n","        use_tqdm=True\n","    )\n","\n","    responses = [x.outputs[0].text for x in responses]\n","    df[\"response\"] = responses\n","\n","    llm_choices = df[\"response\"].astype(int).values - 1\n","\n","    survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n","\n","results = []\n","\n","for i in range(indices.shape[0]):\n","    ix = indices[i]\n","    llm_choice = survivors[i, 0]\n","\n","    results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n","\n","df[\"MisconceptionId\"] = results\n","df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:27:58.781423Z","iopub.execute_input":"2025-01-14T17:27:58.781939Z","iopub.status.idle":"2025-01-14T17:27:58.791765Z","shell.execute_reply.started":"2025-01-14T17:27:58.781893Z","shell.execute_reply":"2025-01-14T17:27:58.790559Z"},"id":"mikM71UHwLai","outputId":"a9bd067f-5757-403d-ec23-8020df576136"},"outputs":[{"name":"stdout","text":"Writing run_vllm.py\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["!python run_vllm.py"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:27:58.793149Z","iopub.execute_input":"2025-01-14T17:27:58.793476Z","iopub.status.idle":"2025-01-14T17:31:05.427406Z","shell.execute_reply.started":"2025-01-14T17:27:58.793450Z","shell.execute_reply":"2025-01-14T17:31:05.426306Z"},"id":"9M1dNROpwLaj","outputId":"6e46ae48-b320-4caf-cb3c-ca392f629dba"},"outputs":[{"name":"stdout","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nWARNING 01-14 17:28:04 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 01-14 17:28:04 config.py:715] Defaulting to use mp for distributed inference\nINFO 01-14 17:28:04 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, use_v2_block_manager=False, enable_prefix_caching=False)\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nINFO 01-14 17:28:05 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:05 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:05 selector.py:54] Using XFormers backend.\nINFO 01-14 17:28:05 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 01-14 17:28:05 selector.py:54] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:07 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:07 utils.py:784] Found nccl from library libnccl.so.2\nINFO 01-14 17:28:07 utils.py:784] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:07 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 01-14 17:28:07 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 01-14 17:28:08 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 01-14 17:28:16 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:16 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 01-14 17:28:16 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7dfaf02ca590>, local_subscribe_port=45609, local_sync_port=34051, remote_subscribe_port=None, remote_sync_port=None)\nINFO 01-14 17:28:16 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:16 model_runner.py:680] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:16 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 01-14 17:28:16 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 01-14 17:28:16 selector.py:54] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:28:16 selector.py:54] Using XFormers backend.\nLoading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards:  20% Completed | 1/5 [00:23<01:33, 23.49s/it]\nLoading safetensors checkpoint shards:  40% Completed | 2/5 [00:49<01:15, 25.16s/it]\nLoading safetensors checkpoint shards:  60% Completed | 3/5 [01:16<00:51, 25.93s/it]\nLoading safetensors checkpoint shards:  80% Completed | 4/5 [01:45<00:27, 27.05s/it]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [02:12<00:00, 26.89s/it]\nLoading safetensors checkpoint shards: 100% Completed | 5/5 [02:12<00:00, 26.41s/it]\n\n\u001b[1;36m(VllmWorkerProcess pid=225)\u001b[0;0m INFO 01-14 17:30:29 model_runner.py:692] Loading model weights took 9.0934 GB\nINFO 01-14 17:30:29 model_runner.py:692] Loading model weights took 9.0934 GB\nINFO 01-14 17:30:38 distributed_gpu_executor.py:56] # GPU blocks: 1399, # CPU blocks: 2048\nExample:\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\nQuestion: \\[\n3 \\times 2+4-5\n\\]\nWhere do the brackets need to go to make the answer equal 13 ?\nCorrect Answer: 3 \\times(2+4)-5 \nIncorrect Answer: 3 \\times 2+(4-5) \n\nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the Incorrect Answer.\nPick the correct misconception number from the below:\n\n1. Believes addition comes before indices, in orders of operation\n2. Done a different calculation to the one given\n3. Subtracts instead of adds\n4. Confuses the order of operations, believes addition comes before division\n5. Performs addition ahead of division\n6. Has not realised that the answer may be changed by the insertion of brackets\n7. Has completed only one of the two operations.\n8. Does not follow the arrows through a function machine, changes the order of the operations asked.\n9. Performs subtraction in wrong order<|im_end|>\n<|im_start|>assistant\n\n\nProcessed prompts: 100%|█| 9/9 [00:05<00:00,  1.50it/s, est. speed input: 471.21\nExample:\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\nQuestion: \\[\n3 \\times 2+4-5\n\\]\nWhere do the brackets need to go to make the answer equal 13 ?\nCorrect Answer: 3 \\times(2+4)-5 \nIncorrect Answer: 3 \\times 2+(4-5) \n\nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the Incorrect Answer.\nPick the correct misconception number from the below:\n\n1. Performs subtraction right to left if priority order means doing a calculation to the right first\n2. Believes order of operations does not affect the answer to a calculation\n3. Performs addition ahead of any other operation\n4. Answers order of operations questions with brackets as if the brackets are not there\n5. Performs addition ahead of subtraction\n6. Does not interpret the correct order of operations from a worded problem\n7. Confuses the order of operations, believes subtraction comes before multiplication \n8. Believes that the order of a worded calculation should be changed to follow BIDMAS \n9. Has not realised that the answer may be changed by the insertion of brackets<|im_end|>\n<|im_start|>assistant\n\n\nProcessed prompts: 100%|█| 9/9 [00:05<00:00,  1.66it/s, est. speed input: 548.05\nExample:\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere is a question about Use the order of operations to carry out calculations involving powers(BIDMAS).\nQuestion: \\[\n3 \\times 2+4-5\n\\]\nWhere do the brackets need to go to make the answer equal 13 ?\nCorrect Answer: 3 \\times(2+4)-5 \nIncorrect Answer: 3 \\times 2+(4-5) \n\nYou are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\nAnswer concisely what misconception it is to lead to getting the Incorrect Answer.\nPick the correct misconception number from the below:\n\n1. Carries out operations from right to left regardless of priority order\n2. Carries out operations from left to right regardless of priority order\n3. Inserts brackets but not changed order of operation\n4. Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\n5. Performs addition ahead of multiplication\n6. Confuses the order of operations, believes addition comes before multiplication \n7. Carries out operations from left to right regardless of priority order, unless brackets are used\n8. May have made a calculation error using the order of operations\n9. Performs subtraction right to left if priority order means doing a calculation to the right first<|im_end|>\n<|im_start|>assistant\n\n\nProcessed prompts: 100%|█| 9/9 [00:05<00:00,  1.57it/s, est. speed input: 517.57\nERROR 01-14 17:31:02 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 225 died, exit code: -15\nINFO 01-14 17:31:02 multiproc_worker_utils.py:123] Killing local vLLM worker processes\n[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["pd.read_csv(\"submission.csv\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:31:05.428995Z","iopub.execute_input":"2025-01-14T17:31:05.429339Z","iopub.status.idle":"2025-01-14T17:31:05.446907Z","shell.execute_reply.started":"2025-01-14T17:31:05.429307Z","shell.execute_reply":"2025-01-14T17:31:05.445637Z"},"id":"e5wboKOfwLal","outputId":"9508c01b-0b23-49ba-c3b8-248123cc9e62"},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"  QuestionId_Answer                                    MisconceptionId\n0            1869_B  1345 706 1507 2306 328 1672 1005 2518 1963 253...\n1            1869_C  1345 2306 1507 706 1005 1999 2488 2532 987 251...\n2            1869_D  315 1005 328 1507 2532 1672 1516 706 1345 2306...\n3            1870_A  891 2142 2068 167 418 1755 979 113 1421 320 22...\n4            1870_B  891 2142 2068 167 341 979 1755 1871 143 418 11...\n5            1870_C  891 2142 2068 167 1755 418 113 2078 143 979 26...\n6            1871_A  1287 1073 2439 1665 2551 1306 1059 1098 1677 1...\n7            1871_C  1287 1073 2439 1665 2551 1098 1059 912 1306 16...\n8            1871_D  1287 1073 1059 1866 903 2471 912 2439 2064 167...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>QuestionId_Answer</th>\n      <th>MisconceptionId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1869_B</td>\n      <td>1345 706 1507 2306 328 1672 1005 2518 1963 253...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1869_C</td>\n      <td>1345 2306 1507 706 1005 1999 2488 2532 987 251...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1869_D</td>\n      <td>315 1005 328 1507 2532 1672 1516 706 1345 2306...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1870_A</td>\n      <td>891 2142 2068 167 418 1755 979 113 1421 320 22...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1870_B</td>\n      <td>891 2142 2068 167 341 979 1755 1871 143 418 11...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1870_C</td>\n      <td>891 2142 2068 167 1755 418 113 2078 143 979 26...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1871_A</td>\n      <td>1287 1073 2439 1665 2551 1306 1059 1098 1677 1...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1871_C</td>\n      <td>1287 1073 2439 1665 2551 1098 1059 912 1306 16...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1871_D</td>\n      <td>1287 1073 1059 1866 903 2471 912 2439 2064 167...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["---"],"metadata":{"id":"hyGDNKauwLal"}},{"cell_type":"markdown","source":["# **Evaluation**"],"metadata":{"id":"5C1j2jYfwLal"}},{"cell_type":"markdown","source":["+ Tính `average precision` cho một cặp nhãn thực tế và dự đoán với giới hạn top-k kết quả.\n","+ Từ đó xác định `mean average precision` (độ chính xác trung bình tổng thể) cho toàn bộ danh sách nhãn thực tế và dự đoán."],"metadata":{"id":"FLFTNCxnwLam"}},{"cell_type":"code","source":["import numpy as np\n","\n","def apk(actual, predicted, k=25):\n","    if not actual:\n","        return 0.0\n","\n","    if len(predicted)>k:\n","        predicted = predicted[:k]\n","\n","    score = 0.0\n","    num_hits = 0.0\n","\n","    for i,p in enumerate(predicted):\n","        # first condition checks whether it is valid prediction\n","        # second condition checks if prediction is not repeated\n","        if p in actual and p not in predicted[:i]:\n","            num_hits += 1.0\n","            score += num_hits / (i+1.0)\n","\n","    return score / min(len(actual), k)\n","\n","def mapk(actual, predicted, k=25):\n","    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:31:05.448222Z","iopub.execute_input":"2025-01-14T17:31:05.448647Z","iopub.status.idle":"2025-01-14T17:31:05.463916Z","shell.execute_reply.started":"2025-01-14T17:31:05.448604Z","shell.execute_reply":"2025-01-14T17:31:05.462820Z"},"id":"cyDLM3cgwLaz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["if not IS_SUBMISSION:\n","    predicted = pd.read_csv(\"submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n","\n","    df_label = {}\n","\n","    for _, row in tqdm(df_ret.iterrows(), total=len(df_ret)):\n","        for option in [\"A\", \"B\", \"C\", \"D\"]:\n","            if (row[\"CorrectAnswer\"] != option) and (row[f\"Misconception{option}Id\"] != -1):\n","                df_label[f\"{row['QuestionId']}_{option}\"] = [row[f\"Misconception{option}Id\"]]\n","\n","    label = df_label.values()\n","\n","    print(\"Validation: \", mapk(label, predicted))"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T17:31:05.465468Z","iopub.execute_input":"2025-01-14T17:31:05.465923Z","iopub.status.idle":"2025-01-14T17:31:05.484559Z","shell.execute_reply.started":"2025-01-14T17:31:05.465876Z","shell.execute_reply":"2025-01-14T17:31:05.483157Z"},"id":"Z1S-9aBmwLa0"},"outputs":[],"execution_count":null}]}